#+TITLE: /Twitter users: What Do They Know? Do They Know Things? Let's Find Out!/ @@latex:\\ {\LARGE Analyzing the accuracy of predictions of public approval of the government using sentiment analysis of Twitter data }@@
#+LANGUAGE: en
#+KEYWORDS: social media, natural language processing, government opinion, mining
#+LATEX_CLASS: IEEEtran
#+LATEX_CLASS_OPTIONS: [conference]
#+LATEX_HEADER: \usepackage[backend=biber,bibencoding=utf8]{biblatex}
#+LATEX_HEADER: \addbibresource{export.bib}
#+LATEX_HEADER: \usepackage{float}
#+OPTIONS: author:nil toc:nil
#+LATEX_HEADER: \author{\IEEEauthorblockN{ Marco Antônio Ribeiro de Toledo \\ RA:\@ 11796419 } \IEEEauthorblockA{ B.Sc. in Computer Science\\ Instituto de Ciências Matemáticas e de Computação\\ University of São Paulo (USP)\\ mardt@usp.br \\ ORCiD: orcid.org/0000-0002-0484-8450}}
#+LATEX_HEADER: \IEEEpeerreviewmaketitle{}

#+BEGIN_abstract
...
#+END_abstract

#+BEGIN_IEEEkeywords
data mining, social media, political analysis, prediction accuracy
#+END_IEEEkeywords

* Related work
With the widespread usage of social media, like the microblogging platform /Twitter/, by the general population, its usage as a barometer for public opinion has been discussed more and more in recent years, even more so in politics, a field where accurate predictions of public opinion are essential. This resulted in ample research being done in the area, from initial work like Tumasjan et al. \cite{Tumasjan2010} in 2010 using Linguistic Inquiry and Word Count to analyze favorability rates of parties and politicians and, therefore, predict election results, to more recent and complex models like Nasrul et al. \cite{Aziz2018} in 2018 using Support Vector Machines to identify public satisfaction with government services, resulting in fairly accurate predictive models.

To support this kind of research, much has been analyzed on the ability to accurately extract sentiment data of /tweets/ at random, with different methods of analysis constantly being tested for their accuracy, from simple semantic scoring of corpuses as in \cite{Kumar2015} to more complex lexicon-based sentence analysis like the work done by Meduru et al. \cite{opinionTwitter}, with recent research showing that the best performing classifier considering accuracy, precision, recall and F1-score is a combination of the more complex and computationally intensive Logistic Regression and Stochastic Gradient Descent as seen in \cite{Yousaf2021}.

In the field of brazilian politics, studies have showed the accuracy of sentiment analysis models when working with /tweets/ from brazilian users on a portuguese corpus, as seen in \cite{Weiand2017}, and others have been conducted in the past on the accuracy of twitter content as a predictor of public opinion as seen in \cite{Oliveira2017} and \cite{Oliveira2019} giving accurate results, although running into limitations.

This approach has also been used by private companies like /Arquimedes/ \cite{Arquimedes2021}, with results comparable to traditional surveys conducted by companies like /Vox Populi/, /IBOPE/ and /Datafolha/, resulting in those analyses now being regularly used by many reputable sources (as seen in \cite{arquimedesMidia}).

** Hypothesis
Taking into account previous research done in the field, our main hypothesis is that opinion data mined straight from a given social media platform (/Twitter/ in our case) is within margin of error of traditional polling, allowing its results to be taken as statically equivalent to the much more costly and time consuming traditional, in person, polling techniques.

# This hypothesis is supported and has its importance reinforced by the increasing presence of social media data mining, as the ones done by /Arquimedes/ \cite{arquimedesMidia}, in the media and the previous work in other contexts that show the reliability of such methods, as the one done in Indonesia in 2018 \cite{Budiharto2018} or even in the context of the German elections in 2010 \cite{Tumasjan2010}, showing that results either confirming or contradicting the trust in this information would be important for the credibility of such reporting.
This hypothesis is supported by previous work in other contexts that show the reliability of such methods in politics, as those done in Indonesia in 2018 \cite{Budiharto2018}, in the context of the french elections in 2017 \cite{Wang2017} and many others \cite{Tumasjan2010}\cite{Kassraie2017}, showing results that closely correspond to traditionally collected data. Also, other studies have already theorized on the positive impact on the popular participation in politics that could be brought to the brazilian political landscape by such data mining \cite{Oliveira2019} \cite{Oliveira2017}.

** Research questions
*** Exploratory questions
- Are there any research done already on the accuracy of political predictions based on /Twitter/ data mining?
    # Yes, there's already a considerable body of work on this kind of comparative analysis \cite{Budiharto2018} \cite{Tumasjan2010} \cite{Wang2017} \cite{Kassraie2017}, only not on the specific context of brazilian politics, which should be solved by this project.
- What is the current state-of-the-art method for this kind of analysis?
    # Based on the latest research, the best method of sentiment analysis is done using Support Vector Machines on the vectorization of the tweet's text, followed by simpler Naive Bayes models \cite{Aziz2018} \cite{Jain2015} \cite{Ramteke2016} \cite{Sharma2016}.
*** Descriptive questions
- Is data mining cheaper/more efficient than traditional polling?
    # Taking electoral polls done professionally in 2018 in Brasil, the costs of running a national poll can run anywhere from R\textdollar 18.95 to R\textdollar 115.50 per person (based on https://www.moneytimes.com.br/quanto-custam-as-pesquisas-eleitorais-veja-as-mais-caras/).
- Does Twitter's user base reflect popular opinion?
    # Based on research done by the platform itself (https://veja.abril.com.br/tecnologia/75-dos-usuarios-brasileiros-tem-no-twitter-uma-fonte-sobre-politica/), only 15\% of brazilian users do not use /Twitter/ as a source of political news.
*** Causality questions
- Can data assertions of the accuracy of predictions on the current government be generalized temporally?

# Kassraie2017 - RNTN: 80% accuracy
* Methodology
The main objective of this study is to compare the accuracy of data mining methods to the traditional polling methods on the public's opinion of the current brazilian administration, so we try to abstract the qualitative data mining classification into a quantitative identification of the public's opinion in a 3-way classification akin to the one done by IPEC in its opinion polling on the approval of the current government (as seen on its monthly press report \cite{IpecSetembro}).

Here we outline the methodological procedure for each step of this study: the data collection, extracting the relevant information from /Twitter/ for analysis; data preprocessing, discarding ambiguous, duplicate or otherwise irrelevant texts; sentiment analysis, extracting the sentiment data of each extracted /tweet/; statistical analysis, comparing the results with ones obtained by traditional polling.
** Data collection
/Twitter/ itself provides its own API with support for keyword based searches, so the information was collected running searches over relevant keywords for our context: /'Bolsonaro', 'governo', 'presidente'/, etc. within the analyzed periods for each comparison, extracting all matches in the space of a given month in 2021.
** Data preprocessing
For a more representative data set, focusing on reducing inconsistencies, redundancies and misleading information in the data, we had to, before starting the sentiment analysis, clear the data set removing:
- /Tweets/ containing URLs, which may indicate an ambiguous text (differentiating if the sentiment expressed refers to the contents of the URL or the subject would require further investigating), detected with regular expressions
- Repeated, non retweeted text, which may indicate content by spam bots, detected by keeping a set of unique tweets
- User handles, anonymizing the data for publishing, replaced using regular expressions
# - /tweets/ in other languages, detected using TextBlob \footnote{https://github.com/sloria/TextBlob}
** Sentiment analysis
Once we had the striped text for each /tweet/ and its related keyword from whose search the data was extracted, we could analyze the sentiment of each instance and tally them to the overall sentiment of each keyword. Due to the limitations in the scope of this paper, the sentiment mining was done using LeIA \cite{Almeida2018} a brazilian-portuguese /fork/ of the lexicon-based sentiment analysis tool VADER \cite{Hutto2014}. The original tool has great accuracy for this kind of analysis considering its 0.96 F1 score on 3-class accuracy for a corpus of /tweets/ while also maintaining good performance due to its lexical nature.
** Statistical analysis
We propose then taking the harmonic mean of these values as an abstraction of the overall opinion on the current brazilian administration, which can be compared to IPEC's public opinion poll on administrative/political subjects (/PESQUISA DE OPINIÃO PÚBLICA SOBRE ASSUNTOS POLÍTICOS/ADMINISTRATIVOS/) \cite{IpecPesquisas} in the respective time period, ideally being within its margin of error of 2 percentage points.

* Results and discussion
The latest compiled public opinion poll on the brazilian government done by IPEC dated June 2021 (from 06-17-2021 to 06-21-2021) \cite{AvaliacaoGovernoIPEC} was used as reference for the analysis, with the /Twitter/ data being collected from the same span of time. The underlying reference question for the classification was /Do you personally approve or disapprove of the way/ President Jair Bolsonaro /is governing Brazil?/, with the following data (0.95 confidence with a margin of error of 0.02):

|            | Approves | Disapproves | Other* | Total |
|------------+----------+-------------+--------+-------|
| Count      |      601 |        1321 |     80 |  2002 |
| Percentage |      30% |         66% |     4% |  100% |
/* either 'doesn't know' or 'didn't answer'./

#+ATTR_LATEX: :placement [H] :width \linewidth
#+CAPTION: Polling results.
#+NAME: fig:polls
[[./ValsIPEC.png]]

#+ATTR_LATEX: :placement [H] :width \linewidth
#+CAPTION: Overral classification of binary responses.
#+NAME: fig:nonneutral
[[./piePoll.png]]

Due to limitations of the free tier /Twitter/ API, only 1750 /tweets/ were collected using the archive search over the corresponding time span, which resulted in a data set of $n = 1524$ usable /tweets/ after the post processing that were later classified using a custom fork of LeIA \footnote{https://github.com/Ocramoi/LeIA}, adapting some features for this specific analysis (this process is also explained in the documentation for the accompanying code).

The classification of the overall sentiment of a /tweet/ was done using the compound score for the analyzed text. As per the original documentation for the tool, /tweets/ with a compound score of over 0.05 were classified as positive, with $m_{positive}$ total tweets, those with a scores lower than -0.05 were classified as negative, with $m_{negative}$ total tweets, and the rest were included as "neutral", totaling $m_{neutral}$ tweets. The percentage of /tweets/ in each category was taken as an estimator for the overall opinion of the population as such:
$$ \hat{p}_{approves} = \frac{m_{positive}}{n} $$
$$ \hat{p}_{disapproves} = \frac{m_{negative}}{n} $$
$$ \hat{p}_{other} = \frac{m_{neutral}}{n} $$
Also, due to the lack of precision in keyword based mining, resulting in an elevated number of neutral tweets, the ratio of approval within those with a binary answer (approves/disapproves) was also estimated, as such:
$$ \hat{p}_{ratio} = \frac{m_{positive}}{n - m_{neutral}} $$

The raw data collected was as follows:
|            | Positive | Negative | Neutral | Total |
|------------+----------+----------+---------+-------|
| Count      |      388 |      836 |     300 |  1524 |
| Percentage |      25% |      55% |     20% |  100% |

#+ATTR_LATEX: :placement [H] :width \linewidth
#+CAPTION: Mining results.
#+NAME: fig:mined
[[./ValsTwitter.png]]

#+ATTR_LATEX: :placement [H] :width \linewidth
#+CAPTION: Overral classification of non-neutral tweets.
#+NAME: fig:nonneutral
[[./pieTweet.png]]

From which we can calculate our biases as follows
$$ V(\hat{p}_{approves}) = \frac{388}{1524} - 0.30 \approx -0.045 $$
$$ V(\hat{p}_{disapproves}) = \frac{836}{1524} - 0.66 \approx -0.111 $$
$$ V(\hat{p}_{other}) = \frac{300}{1524} - 0.04 \approx 0.157 $$
$$ V(\hat{p}_{ratio}) = \frac{388}{1524 - 300} - \frac{601}{2002 - 80} \approx 0.004 $$

This shows a strong bias in favor of neutral responses, with the total number of /tweets/ in each category being a weak indicator for the overall population taking the polling as a standard, with values well over the margin of error. However, the estimated ratio of positive responses over the total number of "opinionated" ones does describe the sample fairly well, reinforcing the idea that the naive data mining over the /tweets/ with only keywords tends to over represent neutral responses.

\printbibliography{}

# TODO: svm processing USING TweetSentBR, acronym expansion, data normalizing, correct spelling
